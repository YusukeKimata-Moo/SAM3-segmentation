{"cells":[{"cell_type":"markdown","metadata":{"id":"r8N0bnnIegJr"},"source":["# **Video Segmentation with SAM 3**\n","\n","Author: Dr.Yusuke Kimata (ORCID: https://orcid.org/0000-0002-1366-0636)<br>\n","ref: Notebook by Meta Research (https://github.com/facebookresearch/sam3/tree/main/examples)<br>\n","Notebook by Roboflow (https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-videos-with-segment-anything-3.ipynb)\n","\n","## Prerequisite\n","Ensure you have a directory named `input` containing your video frames as JPEG files (e.g., `000.jpg`, `001.jpg`, ...).<br>\n","Make sure to change the runtime type to **\"GPU\"**"],"id":"r8N0bnnIegJr"},{"cell_type":"markdown","metadata":{"id":"6ukGnl6YegJv"},"source":["## 1. Environment Setup"],"id":"6ukGnl6YegJv"},{"cell_type":"markdown","metadata":{"id":"T7ksGVqPudcn"},"source":["### Configure your API keys\n","\n","To pull Segment Anything 3 weights, you need a HuggingFace Access Token with approved access to the SAM 3 checkpoints.\n","\n","- Request access to the SAM 3 checkpoints on the official Hugging Face [repo](https://huggingface.co/facebook/sam3).\n","- Open your HuggingFace Settings page. Click Access Tokens then New Token to generate a new token.\n","- In Colab, go to the left pane and click on Secrets (ðŸ”‘). Store your HuggingFace Access Token under the name `HF_TOKEN`.\n","\n"],"id":"T7ksGVqPudcn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTrd2gmzHONO"},"outputs":[],"source":["import os\n","from google.colab import userdata\n","\n","os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"],"id":"cTrd2gmzHONO"},{"cell_type":"markdown","metadata":{"id":"ncCj5PUgxVV6"},"source":["### Check GPU availability\n"],"id":"ncCj5PUgxVV6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILPFDHXR8Cfh"},"outputs":[],"source":["!nvidia-smi"],"id":"ILPFDHXR8Cfh"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"WOa7UjA5eitB"},"id":"WOa7UjA5eitB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchvision\n","\n","print(\"PyTorch version:\", torch.__version__)\n","print(\"Torchvision version:\", torchvision.__version__)\n","print(\"CUDA is available:\", torch.cuda.is_available())"],"metadata":{"id":"7b88-j_ff7MK"},"id":"7b88-j_ff7MK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Install SAM 3 and dependencies\n","\n","If you run this cell, you will be prompted to restart the session. Approve the restart, then proceed to the next cell."],"metadata":{"id":"n3z-jMVrQSwv"},"id":"n3z-jMVrQSwv"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIgsF8VyegJw"},"outputs":[],"source":["import os\n","\n","!pip install -q supervision roboflow\n","\n","# Clone and install SAM 3\n","!git clone https://github.com/facebookresearch/sam3.git # Comment out this line on subsequent runs\n","%cd sam3\n","!pip install -q -e \".[notebooks]\"\n","%cd /content\n"],"id":"xIgsF8VyegJw"},{"cell_type":"markdown","source":["When you run the next cell, a â€œSession crashedâ€ message may appear. You can ignore it and proceed to the next cell."],"metadata":{"id":"MOzxU_KTGKec"},"id":"MOzxU_KTGKec"},{"cell_type":"code","source":["os._exit(0)"],"metadata":{"id":"D4rS0GkhEHlP"},"id":"D4rS0GkhEHlP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q supervision jupyter_bbox_widget"],"metadata":{"id":"JEEe3qWhhBN7"},"id":"JEEe3qWhhBN7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip uninstall -y cc_torch; TORCH_CUDA_ARCH_LIST=\"8.0 9.0\"; pip install git+https://github.com/ronghanghu/cc_torch\n","!pip uninstall -y torch_generic_nms; TORCH_CUDA_ARCH_LIST=\"8.0 9.0\"; pip install git+https://github.com/ronghanghu/torch_generic_nms"],"metadata":{"id":"0mcR5u1fhFln"},"id":"0mcR5u1fhFln","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import torch\n","import os\n","\n","import numpy as np\n","import supervision as sv\n","\n","from pathlib import Path\n","from PIL import Image\n","from typing import Optional\n","from IPython.display import Video\n","import matplotlib.pyplot as plt\n","\n","from jupyter_bbox_widget import BBoxWidget\n","\n","from sam3.model_builder import build_sam3_video_predictor\n","from sam3.visualization_utils import (\n","    prepare_masks_for_visualization,\n","    visualize_formatted_frame_output,\n",")\n","\n","import ipywidgets as widgets\n","from IPython.display import display\n","\n","HOME = Path.cwd()\n","print(\"HOME:\", HOME)"],"metadata":{"id":"uahLOqt8hfib"},"id":"uahLOqt8hfib","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1pHPFsg7egJz"},"outputs":[],"source":["input_name = \"input\"\n","output_name = \"output\"\n","\n","INPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/SAM3/\" + input_name + \"/\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/SAM3/\" + output_name + \"_mask/\"\n","\n","if not os.path.exists(INPUT_DIR):\n","    os.makedirs(INPUT_DIR)\n","    print(f\"Created {INPUT_DIR}. Please upload your frames here.\")\n","    # Create a dummy video frame for testing if empty\n","    img = np.zeros((512, 512, 3), dtype=np.uint8)\n","    cv2.circle(img, (256, 256), 50, (0, 255, 0), -1) # Green circle\n","    cv2.imwrite(os.path.join(INPUT_DIR, \"000.jpg\"), img)\n","    cv2.circle(img, (260, 260), 50, (0, 255, 0), -1) # Moved circle\n","    cv2.imwrite(os.path.join(INPUT_DIR, \"001.jpg\"), img)\n","else:\n","    print(f\"Directory {INPUT_DIR} exists.\")\n","\n","os.makedirs(OUTPUT_DIR, exist_ok=True)"],"id":"1pHPFsg7egJz"},{"cell_type":"markdown","metadata":{"id":"EAUg7VdQegJ0"},"source":["## 2. Configuration & Model Loading"],"id":"EAUg7VdQegJ0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"RI8xQmz_egJ1"},"outputs":[],"source":["# Setup Device\n","DEVICE = [torch.cuda.current_device()]\n","\n","BPE_PATH = \"/content/sam3/sam3/assets/bpe_simple_vocab_16e6.txt.gz\"\n","\n","# Load Model\n","# Pass checkpoint_path, bpe_path, and config_file explicitly as keyword arguments\n","predictor = build_sam3_video_predictor(\n","    bpe_path=BPE_PATH,\n","    gpus_to_use=DEVICE\n",")"],"id":"RI8xQmz_egJ1"},{"cell_type":"markdown","metadata":{"id":"YNFSlgr-egJ1"},"source":["## 3. Data Loading"],"id":"YNFSlgr-egJ1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifgZUcj0egJ2"},"outputs":[],"source":["frame_paths = sorted([os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.jpg', '.jpeg'))])\n","print(f\"Found {len(frame_paths)} frames.\")\n","\n","# Preview first frame\n","if frame_paths:\n","    sv.plot_image(cv2.imread(frame_paths[0]), (5,5))"],"id":"ifgZUcj0egJ2"},{"cell_type":"markdown","metadata":{"id":"dIS2w6wUegJ4"},"source":["## 4. Inference loop"],"id":"dIS2w6wUegJ4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWbAysdGegJ4"},"outputs":[],"source":["response = predictor.handle_request(\n","    request=dict(\n","        type=\"start_session\",\n","        resource_path=Path(INPUT_DIR).as_posix(),\n","    )\n",")\n","session_id = response[\"session_id\"]"],"id":"UWbAysdGegJ4"},{"cell_type":"code","source":["_ = predictor.handle_request(\n","    request=dict(\n","        type=\"reset_session\",\n","        session_id=session_id,\n","    )\n",")"],"metadata":{"id":"f3JicHmxpLHX"},"id":"f3JicHmxpLHX","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_itpWw25egJ5"},"source":["### Adding a text prompt"],"id":"_itpWw25egJ5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZovYlhhegJ5"},"outputs":[],"source":["def load_frame(directory: str, index: int):\n","    \"\"\"\n","    Loads a frame with a specific index from a directory where frames are named\n","    using the pattern '%03d.jpg' (e.g., 000.jpg, 001.jpg, 002.jpg).\n","\n","    Args:\n","        directory (str): Path to the directory containing image frames.\n","        index (int): Frame index (0-based).\n","\n","    Returns:\n","        numpy.ndarray: Loaded frame in BGR format.\n","\n","    Raises:\n","        FileNotFoundError: If the frame does not exist or cannot be read.\n","    \"\"\"\n","    directory_path = Path(directory)\n","    frame_path = directory_path / f\"{index:03d}.jpg\"\n","\n","    if not frame_path.exists():\n","        raise FileNotFoundError(f\"Frame not found: {frame_path}\")\n","\n","    frame = cv2.imread(str(frame_path))\n","    if frame is None:\n","        raise FileNotFoundError(f\"Failed to load frame: {frame_path}\")\n","\n","    return frame\n"],"id":"IZovYlhhegJ5"},{"cell_type":"markdown","source":["Specify the object you want to detect using a TEXT prompt (e.g., \"cell\"). SAM 3 will automatically identify multiple object instances and assign each a unique object ID."],"metadata":{"id":"NNZbELHItqW7"},"id":"NNZbELHItqW7"},{"cell_type":"code","source":["frame_idx = 0\n","frame = load_frame(INPUT_DIR, frame_idx)\n","\n","TEXT = \"cell\""],"metadata":{"id":"s60TwkjuqQVA"},"id":"s60TwkjuqQVA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Segmentation in the reference frame"],"metadata":{"id":"d8PGnE1NxI4Y"},"id":"d8PGnE1NxI4Y"},{"cell_type":"code","source":["response = predictor.handle_request(\n","    request=dict(\n","        type=\"add_prompt\",\n","        session_id=session_id,\n","        frame_index=frame_idx,\n","        text=TEXT,\n","    )\n",")\n","result = response[\"outputs\"]"],"metadata":{"id":"C_nTwlcsypv-"},"id":"C_nTwlcsypv-","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coMzoPtYOvfk"},"source":["### Visualize results"],"id":"coMzoPtYOvfk"},{"cell_type":"code","execution_count":null,"metadata":{"id":"-awmdWmwrpzT"},"outputs":[],"source":["def from_sam(result: dict) -> sv.Detections:\n","    return sv.Detections(\n","        xyxy=sv.mask_to_xyxy(result[\"out_binary_masks\"]),\n","        mask=result[\"out_binary_masks\"],\n","        confidence=result[\"out_probs\"],\n","        tracker_id=result[\"out_obj_ids\"],\n","    )"],"id":"-awmdWmwrpzT"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Hi-piBbsW30"},"outputs":[],"source":["COLOR = sv.ColorPalette.from_hex([\n","    \"#ffff00\", \"#ff9b00\", \"#ff8080\", \"#ff66b2\", \"#ff66ff\", \"#b266ff\",\n","    \"#9999ff\", \"#3399ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n","])\n","\n","def annotate(image: np.ndarray, detections: sv.Detections, text = None) -> np.ndarray:\n","    h, w, _ = image.shape\n","    text_scale = sv.calculate_optimal_text_scale(resolution_wh=(w, h)) * 1.5\n","\n","    mask_annotator = sv.MaskAnnotator(\n","        color=COLOR,\n","        color_lookup=sv.ColorLookup.TRACK,\n","        opacity=0.3\n","    )\n","\n","    annotated_image = image.copy()\n","    annotated_image = mask_annotator.annotate(annotated_image, detections)\n","\n","    if text:\n","        label_annotator = sv.LabelAnnotator(\n","            color=COLOR,\n","            color_lookup=sv.ColorLookup.TRACK,\n","            text_scale=text_scale,\n","            text_color=sv.Color.BLACK,\n","            text_position=sv.Position.TOP_CENTER,\n","            text_offset=(0, -5)\n","        )\n","        labels = [\n","            f\"#{tracker_id} {text} {confidence:.2f}\"\n","            for tracker_id, confidence in zip(detections.tracker_id, detections.confidence)\n","        ]\n","        annotated_image = label_annotator.annotate(annotated_image, detections, labels)\n","\n","    return annotated_image"],"id":"2Hi-piBbsW30"},{"cell_type":"code","execution_count":null,"metadata":{"id":"rxyLMW0tsLx6"},"outputs":[],"source":["detections = from_sam(result)\n","\n","annotated_frame = annotate(frame, detections, TEXT)\n","\n","print(\"Label structure: [OBJECT ID] [TEXT] [CONFIDENCE]\")\n","sv.plot_image(annotated_frame, size=(5, 5))"],"id":"rxyLMW0tsLx6"},{"cell_type":"code","source":["# plt.close(\"all\")\n","# visualize_formatted_frame_output(\n","#     frame_idx,\n","#     frame_paths,\n","#     outputs_list=[prepare_masks_for_visualization({frame_idx: result})],\n","#     titles=[\"SAM 3 Dense Tracking outputs\"],\n","#     figsize=(5, 5),\n","# )"],"metadata":{"id":"ltIN9CUZZFuT"},"id":"ltIN9CUZZFuT","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"batj_4_xb80o"},"source":["### Propagate in video"],"id":"batj_4_xb80o"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4HttWiSb_Hx"},"outputs":[],"source":["def propagate_in_video(predictor, session_id):\n","    frame_outputs = {}\n","    for response in predictor.handle_stream_request(\n","        request=dict(\n","            type=\"propagate_in_video\",\n","            session_id=session_id,\n","        )\n","    ):\n","        frame_outputs[response[\"frame_index\"]] = response[\"outputs\"]\n","\n","    return frame_outputs"],"id":"a4HttWiSb_Hx"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbLRIdINcNEM"},"outputs":[],"source":["frame_outputs = propagate_in_video(predictor=predictor, session_id=session_id)"],"id":"xbLRIdINcNEM"},{"cell_type":"code","source":["import ipywidgets as widgets\n","from IPython.display import display\n","\n","def view_result(frame_idx):\n","    if frame_idx not in frame_outputs:\n","        return\n","\n","    # Load the original frame\n","    try:\n","        frame = load_frame(INPUT_DIR, frame_idx)\n","    except FileNotFoundError:\n","        print(f\"Could not load frame {frame_idx}\")\n","        return\n","\n","    # Get SAM output and convert to detections\n","    output = frame_outputs[frame_idx]\n","    detections = from_sam(output)\n","\n","    # Annotate using the previously defined function\n","    # Use the global TEXT variable if it exists\n","    label_text = TEXT if 'TEXT' in globals() else None\n","    annotated_frame = annotate(frame, detections, label_text)\n","\n","    # Print frame specific results for verification\n","    if len(detections.confidence) > 0:\n","        print(f\"Frame {frame_idx} Results:\")\n","        for tracker_id, confidence in zip(detections.tracker_id, detections.confidence):\n","            print(f\" - ID {tracker_id}: Confidence {confidence:.4f}\")\n","    else:\n","        print(f\"Frame {frame_idx}: No detections.\")\n","\n","    # Display\n","    sv.plot_image(annotated_frame, size=(6, 6))\n","\n","# Create the interactive widget\n","if 'frame_outputs' in globals() and frame_outputs:\n","    indices = sorted(frame_outputs.keys())\n","    min_val, max_val = indices[0], indices[-1]\n","\n","    slider = widgets.IntSlider(\n","        value=min_val,\n","        min=min_val,\n","        max=max_val,\n","        step=1,\n","        description='Frame:'\n","    )\n","\n","    print(\"Interact with the slider to view processed frames:\")\n","    widgets.interact(view_result, frame_idx=slider)\n","else:\n","    print(\"No frame outputs found. Please run the propagation step first.\")"],"metadata":{"id":"YctDXsctrS8C"},"id":"YctDXsctrS8C","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Save results"],"metadata":{"id":"Wcdu_u4In5N9"},"id":"Wcdu_u4In5N9"},{"cell_type":"markdown","source":["### Save as binary mask"],"metadata":{"id":"o3lE8BLOobUg"},"id":"o3lE8BLOobUg"},{"cell_type":"code","source":["# Set to specific ID (e.g., 0, 1) to save only that object's mask.\n","# Set to None to merge all detected objects into one mask.\n","TARGET_OBJECT_ID = None\n","\n","print(f\"Saving binary masks to {OUTPUT_DIR}...\")\n","if TARGET_OBJECT_ID is not None:\n","    print(f\"Targeting Object ID: {TARGET_OBJECT_ID}\")\n","else:\n","    print(\"Targeting: All Objects (Merged)\")\n","\n","# Iterate through processed frames\n","for frame_idx, output in sorted(frame_outputs.items()):\n","    # Get the binary masks (Shape: [N, H, W]) and object IDs\n","    masks = output[\"out_binary_masks\"]\n","    obj_ids = output[\"out_obj_ids\"]\n","\n","    final_mask = None\n","\n","    if TARGET_OBJECT_ID is not None:\n","        # Find index of the target object\n","        indices = np.where(obj_ids == TARGET_OBJECT_ID)[0]\n","        if len(indices) > 0:\n","            final_mask = masks[indices[0]]\n","    else:\n","        # Combine masks for all objects into a single binary mask\n","        if masks.shape[0] > 0:\n","            final_mask = np.any(masks, axis=0)\n","\n","    # Skip if no mask found for this frame\n","    if final_mask is None:\n","        continue\n","\n","    # Convert boolean mask to 8-bit integer (0 -> 0, True -> 255)\n","    mask_image = final_mask.astype(np.uint8) * 255\n","\n","    # Construct filename (e.g., 000.jpg)\n","    file_path = os.path.join(OUTPUT_DIR, f\"{frame_idx:03d}.jpg\")\n","\n","    # Save image\n","    cv2.imwrite(file_path, mask_image)\n","\n","print(f\"Saved mask images.\")"],"metadata":{"id":"Qru34T28xIU0"},"id":"Qru34T28xIU0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save as merged image"],"metadata":{"id":"LtvR9QuNof_G"},"id":"LtvR9QuNof_G"},{"cell_type":"code","source":["# Set to specific ID (e.g., 0, 1) to save only that object's mask.\n","# Set to None to merge all detected objects into one image.\n","# Note: This uses the value set in the previous cell unless redefined here.\n","# TARGET_OBJECT_ID = 0\n","\n","# Define the output directory for merged images\n","MERGED_DIR = \"/content/drive/MyDrive/Colab Notebooks/SAM3/\" + output_name + \"_merged/\"\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(MERGED_DIR, exist_ok=True)\n","\n","print(f\"Saving merged frames to {MERGED_DIR}...\")\n","\n","# Ensure TARGET_OBJECT_ID exists\n","if 'TARGET_OBJECT_ID' not in globals():\n","    TARGET_OBJECT_ID = None\n","\n","if TARGET_OBJECT_ID is not None:\n","    print(f\"Targeting Object ID: {TARGET_OBJECT_ID}\")\n","else:\n","    print(\"Targeting: All Objects\")\n","\n","# Initialize MaskAnnotator with fixed opacity\n","mask_annotator = sv.MaskAnnotator(\n","    color=COLOR,\n","    color_lookup=sv.ColorLookup.TRACK,\n","    opacity=0.3\n",")\n","\n","# Iterate through all processed frames\n","for frame_idx, output in sorted(frame_outputs.items()):\n","    # Load the original frame using the helper function\n","    try:\n","        frame = load_frame(INPUT_DIR, frame_idx)\n","    except FileNotFoundError:\n","        print(f\"Warning: Could not load frame {frame_idx}, skipping.\")\n","        continue\n","\n","    # Convert the SAM output to supervision Detections\n","    detections = from_sam(output)\n","\n","    # Filter detections if a specific object ID is targeted\n","    if TARGET_OBJECT_ID is not None:\n","        detections = detections[detections.tracker_id == TARGET_OBJECT_ID]\n","\n","    # Annotate the frame with the masks using our custom annotator\n","    merged_frame = mask_annotator.annotate(frame.copy(), detections)\n","\n","    # Construct the output filename (e.g., 000.jpg)\n","    save_path = os.path.join(MERGED_DIR, f\"{frame_idx:03d}.jpg\")\n","\n","    # Save the merged image\n","    cv2.imwrite(save_path, merged_frame)\n","\n","print(f\"Successfully saved {len(frame_outputs)} merged images.\")"],"metadata":{"id":"d85DDn-fzZZR"},"id":"d85DDn-fzZZR","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Close session\n","\n","Each session is tied to a single video. We can close the session after inference to free up its resources.\n","\n","(Then, you may start a new session on another video.)"],"metadata":{"id":"wjWK1FM9vkzt"},"id":"wjWK1FM9vkzt"},{"cell_type":"code","source":["_ = predictor.handle_request(\n","    request=dict(\n","        type=\"close_session\",\n","        session_id=session_id,\n","    )\n",")\n","\n","predictor.shutdown()"],"metadata":{"id":"POSSMKEwvYNO"},"id":"POSSMKEwvYNO","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}